{"cells":[{"cell_type":"markdown","metadata":{"id":"hrsaDfdVHzxt"},"source":["# Custom Training with YOLOv5\n","\n","In this tutorial, we assemble a dataset and train a custom YOLOv5 model to recognize the objects in our dataset. To do so we will take the following steps:\n","\n","* Gather a dataset of images and label our dataset\n","* Export our dataset to YOLOv5\n","* Train YOLOv5 to recognize the objects in our dataset\n","* Evaluate our YOLOv5 model's performance\n","* Run test inference to view our model at work\n","\n","\n","\n","![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"]},{"cell_type":"markdown","metadata":{"id":"yNveqeA1KXGy"},"source":["# AI 신경망 훈련하기(TRAIN)"]},{"cell_type":"markdown","metadata":{"id":"NgbQ5a3cgTOf"},"source":["## 1. yolov5 설치하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14667,"status":"ok","timestamp":1647050866926,"user":{"displayName":"정태균","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04212359468223276880"},"user_tz":-540},"id":"kTvDNSILZoN9","outputId":"3052cc6e-86cf-432d-b103-e98151dcde3a"},"outputs":[{"output_type":"stream","name":"stderr","text":["YOLOv5 🚀 v6.1-28-gc6b4f84 torch 1.10.0+cu111 CPU\n"]},{"output_type":"stream","name":"stdout","text":["Setup complete ✅ (2 CPUs, 12.7 GB RAM, 39.5/107.7 GB disk)\n","Setup complete. Using torch 1.10.0+cu111 (CPU)\n"]}],"source":["#clone YOLOv5 and \n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt # install dependencies\n","# %pip install -q roboflow\n","\n","from yolov5 import utils\n","display = utils.notebook_init()  # checks\n","\n","import torch\n","import os\n","from IPython.display import Image, clear_output  # to display images\n","\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"]},{"cell_type":"markdown","metadata":{"id":"MqWn603ggTOi"},"source":["## 2. 미리 만들어 놓은 학습할 dataset을 colab 왼쪽에 업로드 하기"]},{"cell_type":"markdown","metadata":{"id":"vqUfl2FzgTOj"},"source":["## 3. dataset 압축화일 풀기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_pl9gdXOly5"},"outputs":[],"source":["!unzip -q ../helmet_dataset.zip -d ../ "]},{"cell_type":"markdown","source":["## 4. 미리 만들어진 helmet.yaml 을 data 아래에 다운로드 한다."],"metadata":{"id":"1dOGRwWKiO7Y"}},{"cell_type":"markdown","metadata":{"id":"5nTsUPAHgTOk"},"source":["## 5. 주어진 data 학습하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eE5KemxcZels","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647050652032,"user_tz":-540,"elapsed":157984,"user":{"displayName":"정태균","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04212359468223276880"}},"outputId":"13b65e6e-3223-4f62-9304-ff6297e84e90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=helmet.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=2, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n","YOLOv5 🚀 v6.1-28-gc6b4f84 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n","100% 14.1M/14.1M [00:00<00:00, 109MB/s] \n","\n","Overriding model.yaml nc=80 with nc=1\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model Summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.8 GFLOPs\n","\n","Transferred 343/349 items from yolov5s.pt\n","Scaled weight_decay = 0.0005\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/../labels/train' images and labels...80 found, 0 missing, 0 empty, 3 corrupt: 100% 80/80 [00:00<00:00, 1048.77it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/yolov5/../images/train/000007_jpg.rf.opQFqLLRWdXyJqBTFkcA.jpg: ignoring corrupt image/label: negative label values [         -1          -1]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/yolov5/../images/train/000022_jpg.rf.2u4JRGwIyOHGV5QwMnLN.jpg: ignoring corrupt image/label: negative label values [         -1]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/yolov5/../images/train/000022_jpg.rf.zbOr9jBvqk5PXEXf0yTL.jpg: ignoring corrupt image/label: negative label values [         -1]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov5/../labels/train.cache\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100% 77/77 [00:00<00:00, 212.74it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/../labels/val' images and labels...10 found, 0 missing, 0 empty, 0 corrupt: 100% 10/10 [00:00<00:00, 344.43it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov5/../labels/val.cache\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 10/10 [00:00<00:00, 203.41it/s]\n","Plotting labels to runs/train/exp/labels.jpg... \n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.70 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp\u001b[0m\n","Starting training for 10 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       0/9    0.501G    0.1143   0.03666         0         3       640: 100% 39/39 [00:13<00:00,  2.84it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  5.42it/s]\n","                 all         10         63     0.0581     0.0159    0.00339   0.000629\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       1/9    0.642G    0.1066   0.03931         0         2       640: 100% 39/39 [00:11<00:00,  3.54it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  7.00it/s]\n","                 all         10         63     0.0243     0.0635    0.00564    0.00103\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       2/9    0.642G   0.09645   0.03885         0        10       640: 100% 39/39 [00:10<00:00,  3.56it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  8.03it/s]\n","                 all         10         63      0.318     0.0476     0.0268    0.00496\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       3/9    0.642G   0.09216   0.03837         0         4       640: 100% 39/39 [00:10<00:00,  3.56it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  8.28it/s]\n","                 all         10         63      0.187      0.111     0.0417     0.0134\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       4/9    0.642G   0.09037   0.03188         0         5       640: 100% 39/39 [00:10<00:00,  3.55it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  8.87it/s]\n","                 all         10         63      0.105     0.0635      0.019    0.00365\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       5/9    0.642G   0.08661   0.03522         0         5       640: 100% 39/39 [00:11<00:00,  3.53it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  9.33it/s]\n","                 all         10         63      0.106      0.127      0.036    0.00535\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       6/9    0.642G   0.08586   0.03851         0         3       640: 100% 39/39 [00:11<00:00,  3.49it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  9.30it/s]\n","                 all         10         63      0.182     0.0794     0.0415    0.00621\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       7/9    0.642G   0.08168   0.03928         0        10       640: 100% 39/39 [00:11<00:00,  3.49it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  9.45it/s]\n","                 all         10         63      0.247      0.159     0.0707     0.0161\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       8/9    0.644G   0.07924   0.03955         0         2       640: 100% 39/39 [00:11<00:00,  3.51it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  9.37it/s]\n","                 all         10         63      0.344      0.206      0.117     0.0259\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","       9/9    0.644G    0.0753   0.03729         0         2       640: 100% 39/39 [00:11<00:00,  3.48it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  9.80it/s]\n","                 all         10         63      0.375      0.206      0.164     0.0485\n","\n","10 epochs completed in 0.034 hours.\n","Optimizer stripped from runs/train/exp/weights/last.pt, 14.5MB\n","Optimizer stripped from runs/train/exp/weights/best.pt, 14.5MB\n","\n","Validating runs/train/exp/weights/best.pt...\n","Fusing layers... \n","Model Summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:00<00:00,  3.83it/s]\n","                 all         10         63      0.378      0.206      0.164     0.0485\n","Results saved to \u001b[1mruns/train/exp\u001b[0m\n"]}],"source":["# Train YOLOv5s on COCO128 for 100 epochs\n","!python train.py --img 640 --batch 2 --epochs 10 --data helmet.yaml --weights yolov5s.pt --cache"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"1. YOLOv5 AI 신경망 훈련하기(train.py, 2022.3.12 완료).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}